import queue import sys import time import numpy as np import sounddevice as sd import webrtcvad from faster_whisper import WhisperModel # ---------------------------- # Configuraci√≥n general # ---------------------------- SAMPLE_RATE = 16000 # Whisper trabaja muy bien a 16kHz CHANNELS = 1 FRAME_MS = 30 # webrtcvad soporta 10/20/30 ms FRAME_SAMPLES = int(SAMPLE_RATE * FRAME_MS / 1000) VAD_AGGRESSIVENESS = 2 # 0-3 (3 = m√°s estricto) MAX_UTTERANCE_SEC = 15 # corta segmentos demasiado largos SILENCE_END_MS = 800 # si hay este silencio, finaliza frase MIN_UTTERANCE_MS = 300 # ignora segmentos muy cortos # Whisper model: # tiny / base / small / medium / large-v3 MODEL_SIZE = "small" # para CPU suele ir bien small o base/tiny DEVICE = "cpu" # "cuda" si tienes GPU NVIDIA COMPUTE_TYPE = "int8" # int8 acelera en CPU, en GPU podr√≠a ser "float16" # Idioma: "es" para espa√±ol o None para auto-detecci√≥n LANGUAGE = "es" audio_q = queue.Queue() def list_input_devices(): print("\nDispositivos de audio disponibles:") devices = sd.query_devices() for i, d in enumerate(devices): if d["max_input_channels"] > 0: print(f" [{i}] {d['name']} (inputs: {d['max_input_channels']})") print() def audio_callback(indata, frames, time_info, status): if status: print(status, file=sys.stderr) # indata: float32 [-1, 1], convertimos a int16 PCM (webrtcvad requiere PCM 16-bit mono) pcm16 = (indata[:, 0] * 32768).astype(np.int16) audio_q.put(pcm16.tobytes()) def bytes_to_np_int16(b): return np.frombuffer(b, dtype=np.int16) def run(): print("Cargando WhisperModel... (puede tardar la primera vez)") model = WhisperModel(MODEL_SIZE, device=DEVICE, compute_type=COMPUTE_TYPE) vad = webrtcvad.Vad(VAD_AGGRESSIVENESS) speech_bytes = bytearray() in_speech = False silence_ms = 0 utterance_ms = 0 print("\nüéôÔ∏è Iniciando captura de micr√≥fono (Ctrl+C para salir)\n") with sd.InputStream( samplerate=SAMPLE_RATE, channels=CHANNELS, dtype="float32", blocksize=FRAME_SAMPLES, callback=audio_callback ): while True: frame = audio_q.get() # bytes PCM int16 # webrtcvad requiere frames exactos (10/20/30ms) is_speech = vad.is_speech(frame, SAMPLE_RATE) if is_speech: if not in_speech: in_speech = True # Inicio de frase speech_bytes = bytearray() silence_ms = 0 utterance_ms = 0 speech_bytes.extend(frame) utterance_ms += FRAME_MS # Limita longitud m√°xima por seguridad/latencia if utterance_ms >= MAX_UTTERANCE_SEC * 1000: transcribe_segment(model, speech_bytes) in_speech = False silence_ms = 0 utterance_ms = 0 else: if in_speech: silence_ms += FRAME_MS utterance_ms += FRAME_MS # guardamos tambi√©n un poquito de silencio final para naturalidad (opcional) speech_bytes.extend(frame) # Si el silencio supera el umbral, finalizamos el segmento if silence_ms >= SILENCE_END_MS: # Verifica longitud m√≠nima (evita ruido/clics) if utterance_ms >= MIN_UTTERANCE_MS: transcribe_segment(model, speech_bytes) in_speech = False silence_ms = 0 utterance_ms = 0 def transcribe_segment(model, pcm_bytes): # Convertimos a numpy float32 [-1,1] para faster-whisper audio_int16 = bytes_to_np_int16(pcm_bytes) audio_float32 = (audio_int16.astype(np.float32) / 32768.0) t0 = time.time() segments, info = model.transcribe( audio_float32, language=LANGUAGE, vad_filter=False, # ya usamos VAD externo beam_size=5 ) text = "" for seg in segments: text += seg.text text = text.strip() dt = time.time() - t0 if text: print(f"üìù {text} (latencia: {dt:.2f}s)") else: # En ocasiones el VAD detecta voz pero Whisper no produce texto print(f"‚Ä¶ (sin texto, {dt:.2f}s)") if name == "main": # Si quieres elegir micr√≥fono espec√≠fico, descomenta: # list_input_devices() # sd.default.device = (input_device_index, None) try: run() except KeyboardInterrupt: print("\nSaliendo...")
